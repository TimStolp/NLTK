{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most occcuring non-trivial words:\n",
      "\n",
      "['came', 'Edmund', 'with', 'said', 'room', 'that', 'There', 'them', 'into', 'were', 'house', 'Lucy', 'they']\n"
     ]
    }
   ],
   "source": [
    "## Basic description (in progress)\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/home/tim/Desktop/TTTV/NLTK Project'\n",
    "\n",
    "\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*') \n",
    "wordlists.fileids()\n",
    "Narnia = wordlists.words('Narnia')\n",
    "\n",
    "fdist1 = FreqDist(Narnia)\n",
    "#print(fdist1.most_common(10))\n",
    "print(\"most occcuring non-trivial words:\")\n",
    "print()\n",
    "print(list(set([word for word in Narnia if len(word)>3 and fdist1[word] > 5])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to check if sentence can be parsed with parser of choice\n",
    "\n",
    "from nltk import CFG\n",
    "from nltk.grammar import FeatureGrammar\n",
    "from nltk.parse import RecursiveDescentParser, FeatureEarleyChartParser, ShiftReduceParser\n",
    "from nltk.parse.generate import generate\n",
    "\n",
    "# Function that works for multiple types of parsers (You are free to use something else if you want.)\n",
    "def check_sentence(parser, sentence):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"Checking if provided sentence matches the grammar:\")\n",
    "    print(sentence)\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    tree_found = False\n",
    "    results = parser.parse(sentence)\n",
    "    for tree in results:\n",
    "        tree_found = True\n",
    "        # print(tree)\n",
    "    if not tree_found:\n",
    "        print(sentence, \"Does not match the provided grammar.\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return tree_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The actual context-free grammar \n",
    "\n",
    "cfg = FeatureGrammar.fromstring(\"\"\"\n",
    "    \n",
    "    # sentences\n",
    "    S -> NP VP\n",
    "    S -> VP\n",
    "    S -> ADVP VP\n",
    "    S -> S WHNP\n",
    "    S -> S Comma CC S\n",
    "    S -> S CC S\n",
    "     \n",
    "    SBAR -> WHNP S \n",
    "     \n",
    "    # constituents\n",
    "    NP -> Det N\n",
    "    NP -> Det N N\n",
    "    NP -> N N\n",
    "    NP -> NNP\n",
    "    NP -> Pos N\n",
    "    NP -> ADJP N\n",
    "    NP -> Det ADJP N\n",
    "    NP -> ADJP NP\n",
    "    NP -> NP Comma NP\n",
    "    NP -> Pro\n",
    "    NP -> NP SBAR\n",
    "    NP -> NN SBAR\n",
    "    NP -> NP PP\n",
    "    NP -> NP CC NP\n",
    "    NP -> Adjs\n",
    "    \n",
    "    VP -> V\n",
    "    VP -> V NP\n",
    "    VP -> V ADVP PP\n",
    "    VP -> V ADJP PP\n",
    "    VP -> V Adv VP\n",
    "    VP -> V NP PP\n",
    "    VP -> V NP ADVP\n",
    "    VP -> V PP\n",
    "    VP -> V PP ADVP\n",
    "    VP -> V VP\n",
    "    \n",
    "    PP -> TO NP\n",
    "    PP -> IN NP\n",
    "    PP -> IN IN NP\n",
    "    PP -> Prep NP\n",
    "    PP -> PP CC PP\n",
    "    PP -> PP PP\n",
    "    \n",
    "    ADVP -> Adv\n",
    "    ADVP -> Adv Adv\n",
    "\n",
    "    ADJP -> Adj N\n",
    "    ADJP -> Adj Pro\n",
    "    ADJP -> IterAdj\n",
    "    ADJP -> Adv Adj\n",
    "    ADJP -> Adj\n",
    "    ADJP -> Adj Adj\n",
    "    \n",
    "    WHADVP -> WhAdv\n",
    "    \n",
    "    WHNP -> Pos N VP\n",
    "    WHNP -> Det\n",
    "    WHNP -> WDT\n",
    "    \n",
    "    \n",
    "    # lexicon\n",
    "    Det ->  'the' | 'an' | 'this' | 'that' | 'no' | 'a'\n",
    "    V -> 'were' | 'sent' | 'is' | 'happened' | 'lived' | 'had' | 'come' \\\n",
    "         | 'called' | 'was' | 'do' | 'grew' | 'liked'\n",
    "    Adj -> 'four' | 'old' | 'ten' | 'nearest' | 'two' | 'large' \\\n",
    "            | 'three' | 'mrs' | 'shaggy' | 'white' | 'first'\n",
    "    Adv -> 'away' | 'there' | 'very' | 'not' |  'much' | 'almost' | 'immediately'\n",
    "    IN -> 'from' | 'during' | 'because' | 'of' | 'in' | 'with' | 'into' | 'over' \\\n",
    "           | 'on'\n",
    "    TO -> 'to'\n",
    "    N -> 'children' | 'names' | 'war' | 'airraids' | 'house' | 'professor' \\\n",
    "          | 'story' | 'heart' | 'country' | 'post' | 'office' | 'railway' | 'station' \\\n",
    "          | 'miles' | 'wife' | 'housekeeper' | 'servants' | 'man' | 'face' | 'hair' \\\n",
    "          | 'head' | 'evening'\n",
    "    NNP -> 'london'\n",
    "    NN -> 'something'\n",
    "    Pos -> 'whose' | 'their' | 'his'\n",
    "    Pro -> 'peter' | 'susan' | 'edmund' | 'lucy' | 'they' | 'them' | 'he' \\\n",
    "           | 'ivy' | 'margaret' | 'betty' | 'macready' | 'him'\n",
    "    CC -> 'and' | 'but'\n",
    "    Comma -> ','\n",
    "    Prep -> 'about'\n",
    "    WDT -> 'which'\n",
    "    Adjs -> 'most'\n",
    "    WhAdv -> 'when'\n",
    "    PR -> 'out'\n",
    "    \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the children were\n",
      "the children sent\n",
      "the children is\n",
      "the children happened\n"
     ]
    }
   ],
   "source": [
    "## generating some senteces in non-random fashion\n",
    "\n",
    "for sentence in generate(cfg, n=4):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "they liked him almost immediately\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if sentences can be parsed\n",
    "\n",
    "cfg_parser = FeatureEarleyChartParser(cfg)\n",
    "#check_sentence(cfg_parser, 'There were four children whose names were Peter , edmund , lucy and susan'.lower())\n",
    "#check_sentence(cfg_parser, 'this story is about something that happened to them'.lower())\n",
    "#check_sentence(cfg_parser, 'they were sent away from London during the war because of the air-raids'.lower())\n",
    "#check_sentence(cfg_parser, 'They were sent to the house of an old Professor'.lower())\n",
    "#check_sentence(cfg_parser, 'The Professor lived in the heart of the country'.lower())\n",
    "#check_sentence(cfg_parser, 'he lived ten miles from the nearest railway station and he lived two miles from the nearest post office'.lower())\n",
    "#check_sentence(cfg_parser, 'Their names were Ivy , Margaret and Betty , but they do not come into the story much'.lower())\n",
    "#check_sentence(cfg_parser, 'the housekeeper was a very old man'.lower())\n",
    "check_sentence(cfg_parser, 'they liked him almost immediately'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(grammar, prod, frags):        \n",
    "    if prod in grammar._lhs_index: # Derivation\n",
    "        derivations = grammar._lhs_index[prod]            \n",
    "        derivation = random.choice(derivations)            \n",
    "        for d in derivation._rhs:            \n",
    "            generate_sample(grammar, d, frags)\n",
    "    elif prod in grammar._rhs_index:\n",
    "        # terminal\n",
    "        frags.append(str(prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "frags = []  \n",
    "generate_sample(cfg, cfg.start(), frags)\n",
    "print( ' '.join(frags) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['There were four children whose names were Peter , Susan , Edmund and Lucy'], ['this story is about something that happened to them'], ['they were sent away from London during the war because of the airraids'], ['They were sent to the house of an old Professor'], ['The Professor lived in the heart of the country'], ['he lived ten miles from the nearest railway station and he lived two miles from the nearest post office'], ['He had no wife and he lived in a very large house with a housekeeper and three servants'], ['Their names were Ivy , Margaret and Betty , but they do not come into the story much'], ['The housekeeper was called Mrs Macready'], ['The professor was a very old man with shaggy white hair which grew over most of his face and on his head'], ['they liked him almost immediately'], ['but on the first evening when he came out to meet them at the front door he was so odd - looking that Lucy was a little afraid of him'], ['and Edmund wanted to laugh and had to keep pretending he was blowing his nose to hide it'], ['they had said good night to the Professor and had gone upstairs on the first night'], [\"the boys came into the girls ' room and they all talked it over\"], ['peter said they had fallen on their feet and it was going to be perfectlty splendid'], ['That old chap will let us do anything we like'], ['Susan said she thought he was an old dear'], ['Edmund said to come off it'], ['he was tired and he was pretending not to be tired'], ['that always made him bad - tempered'], ['Do not go on talking like that'], ['Like what ? susan said it is time they were in bed'], [\"Edmund said she was trying to talk like mother And who are you to say when I ' m to go to bed ? Go to bed yourself\"], ['Lucy said they had all better gone to bed'], [\"There is going to be a row if we ' re heard talking here\"], ['peter said there would not be a row'], ['he told them this was the sort of house where no one was going to mind what they did'], ['Anyway , they will not hear us'], ['It is about a ten - minute walk from here down to that dining - room , and ther is any amount of stairs and passages in between'], ['\" What \\' s that noise ?\" said Lucy suddenly'], ['It was a far larger house than she had ever been in before and the thought of all those long passages and rows of doors leading into empty rooms was beginning to make her feel a little creepy'], ['\" It \\' s only a bird , silly ,\" said Edmund'], ['\" It \\' s an owl ,\" said Peter'], ['\" This is going to be a wonderful place for birds'], ['I shall go to bed now'], [\"I say , let ' s go and explore tomorrow\"], ['You might find anything in a place like this'], ['Did you see those mountains as we came along ? And the woods ? There might be eagles'], ['There might be stags'], ['There \\' ll be hawks .\" \" Badgers !\" said Lucy'], ['\" Foxes !\" said Edmund'], ['\" Rabbits !\" said Susan'], ['But when next morning came there was a steady rain falling , so thick that when you looked out of the window you could see neither the mountains nor the woods nor even the stream in the garden'], ['\" Of course it would be raining !\" said Edmund'], ['They had just finished their breakfast with the Professor and were upstairs in the room he had set apart for them - a long , low room with two windows looking out in one direction and two in another'], ['\" Do stop grumbling , Ed ,\" said Susan'], ['\" Ten to one it \\' ll clear up in an hour or so'], [\"And in the meantime we ' re pretty well off\"], ['There \\' s a wireless and lots of books .\" \" Not for me \" said Peter ; \" I \\' m going to explore in the house .\" Everyone agreed to this and that was how the adventures began'], ['It was the sort of house that you never seem to come to the end of , and it was full of unexpected places'], ['The first few doors they tried led only into spare bedrooms , as everyone had expected that they would ; but soon they came to a very long room full of pictures and there they found a suit of armour ; and after that was a room all hung with green , with a harp in one corner ; and then came three steps down and five steps up , and then a kind of little upstairs hall and a door that led out on to a balcony , and then a whole series of rooms that led into each other and were lined with books - most of them very old books and some bigger than a Bible in a church'], ['And shortly after that they looked into a room that was quite empty except for one big wardrobe ; the sort that has a looking - glass in the door'], ['There was nothing else in the room at all except a dead blue - bottle on the window - sill'], ['\" Nothing there !\" said Peter , and they all trooped out again - all except Lucy'], ['She stayed behind because she thought it would be worth while trying the door of the wardrobe , even though she felt almost sure that it would be locked'], ['To her surprise it opened quite easily , and two moth - balls dropped out'], ['Looking into the inside , she saw several coats hanging up - mostly long fur coats'], ['There was nothing Lucy liked so much as the smell and feel of fur'], ['She immediately stepped into the wardrobe and got in among the coats and rubbed her face against them , leaving the door open , of course , because she knew that it is very foolish to shut oneself into any wardrobe'], ['Soon she went further in and found that there was a second row of coats hanging up behind the first one']]\n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "sentence_list = []\n",
    "\n",
    "for word in Narnia:\n",
    "    if not word == '.':\n",
    "        sentence += word + ' '\n",
    "    else:\n",
    "        sentence = sentence[:-1]\n",
    "        sentence_list.append([sentence])\n",
    "        sentence = ''\n",
    "        \n",
    "print(sentence_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "there were four children whose names were peter , susan , edmund and lucy\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "this story is about something that happened to them\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "they were sent away from london during the war because of the airraids\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "they were sent to the house of an old professor\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "the professor lived in the heart of the country\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "he lived ten miles from the nearest railway station and he lived two miles from the nearest post office\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "he had no wife and he lived in a very large house with a housekeeper and three servants\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "their names were ivy , margaret and betty , but they do not come into the story much\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "the housekeeper was called mrs macready\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "the professor was a very old man with shaggy white hair which grew over most of his face and on his head\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "they liked him almost immediately\n",
      "--------------------------------------------------\n",
      "True\n",
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "but on the first evening when he came out to meet them at the front door he was so odd - looking that lucy was a little afraid of him\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'came', 'meet', 'at', 'front', 'door', 'so', 'odd', '-', 'looking', 'little', 'afraid'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-e6bc3f0083ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-682f90858f3c>\u001b[0m in \u001b[0;36mcheck_sentence\u001b[0;34m(parser, sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtree_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtree_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nltk/parse/earleychart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[0;32m--> 648\u001b[0;31m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'came', 'meet', 'at', 'front', 'door', 'so', 'odd', '-', 'looking', 'little', 'afraid'\"."
     ]
    }
   ],
   "source": [
    "for [sentence] in sentence_list:\n",
    "    print(check_sentence(cfg_parser, sentence.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/home/tim/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/local/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-2653ec6eee4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/home/tim/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/local/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(corpus_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
