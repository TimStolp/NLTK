{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most occcuring non-trivial words:\n",
      "\n",
      "['them', 'into', 'house', 'said', 'There', 'Edmund', 'with', 'Lucy', 'they', 'that', 'were', 'came', 'room']\n"
     ]
    }
   ],
   "source": [
    "## Basic description (in progress)\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/home/willem/Documents/TTV'\n",
    "\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*') \n",
    "wordlists.fileids()\n",
    "Narnia = wordlists.words('Narnia')\n",
    "\n",
    "fdist1 = FreqDist(Narnia)\n",
    "#print(fdist1.most_common(10))\n",
    "print(\"most occcuring non-trivial words:\")\n",
    "print()\n",
    "print(list(set([word for word in Narnia if len(word)>3 and fdist1[word] > 5])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to check if sentence can be parsed with parser of choice\n",
    "\n",
    "from nltk import CFG\n",
    "from nltk.grammar import FeatureGrammar\n",
    "from nltk.parse import RecursiveDescentParser, FeatureEarleyChartParser, ShiftReduceParser\n",
    "from nltk.parse.generate import generate\n",
    "\n",
    "# Function that works for multiple types of parsers (You are free to use something else if you want.)\n",
    "def check_sentence(parser, sentence):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"Checking if provided sentence matches the grammar:\")\n",
    "    print(sentence)\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    tree_found = False\n",
    "    results = parser.parse(sentence)\n",
    "    for tree in results:\n",
    "        tree_found = True\n",
    "        print(tree)\n",
    "    if not tree_found:\n",
    "        print(sentence, \"Does not match the provided grammar.\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return tree_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The actual context-free grammar \n",
    "\n",
    "cfg = FeatureGrammar.fromstring(\"\"\"\n",
    "    \n",
    "    # sentences\n",
    "    S -> NP VP\n",
    "    S -> VP\n",
    "    S -> ADVP VP\n",
    "    S -> S WHNP\n",
    "    S -> S Comma CC S\n",
    "    S -> S CC S\n",
    "     \n",
    "    SBAR -> WHNP S \n",
    "     \n",
    "    # constituents\n",
    "    NP -> Det N\n",
    "    NP -> Det N N\n",
    "    NP -> N N\n",
    "    NP -> NNP\n",
    "    NP -> Pos N\n",
    "    NP -> ADJP N\n",
    "    NP -> Det ADJP N\n",
    "    NP -> ADJP NP\n",
    "    NP -> NP Comma NP\n",
    "    NP -> Pro\n",
    "    NP -> NP SBAR\n",
    "    NP -> NN SBAR\n",
    "    NP -> NP PP\n",
    "    NP -> NP CC NP\n",
    "    NP -> JJS\n",
    "    \n",
    "    VP -> V NP\n",
    "    VP -> V ADVP PP\n",
    "    VP -> V ADJP PP\n",
    "    VP -> V Adv VP\n",
    "    VP -> V NP PP\n",
    "    VP -> V PP\n",
    "    VP -> V PP ADVP\n",
    "    VP -> V VP\n",
    "    \n",
    "    PP -> TO NP\n",
    "    PP -> IN NP\n",
    "    PP -> IN IN NP\n",
    "    PP -> Prep NP\n",
    "    PP -> PP CC PP\n",
    "    PP -> PP PP\n",
    "    \n",
    "    ADVP -> Adv\n",
    "\n",
    "    ADJP -> Adj N\n",
    "    ADJP -> Adj Pro\n",
    "    ADJP -> IterAdj\n",
    "    ADJP -> Adv Adj\n",
    "    \n",
    "    IterAdj -> IterAdj IterAdj\n",
    "    IterAdj -> Adj\n",
    "    IterAdj -> IterAdj Comma IterAdj\n",
    "    \n",
    "    \n",
    "    WHNP -> Pos N VP\n",
    "    WHNP -> Det\n",
    "    WHNP -> WDT\n",
    "    \n",
    "    \n",
    "    # lexicon\n",
    "    Det ->  'the' | 'an' | 'this' | 'that' | 'no' | 'a'\n",
    "    V -> 'were' | 'sent' | 'is' | 'happened' | 'lived' | 'had' | 'come' \\\n",
    "         | 'called' | 'was' | 'do' | 'grew'\n",
    "    Adj -> 'four' | 'old' | 'ten' | 'nearest' | 'two' | 'large' \\\n",
    "            | 'three' | 'mrs.' | 'shaggy' | 'white'\n",
    "    Adv -> 'away' | 'there' | 'very' | 'not' |  'much'\n",
    "    IN -> 'from' | 'during' | 'because' | 'of' | 'in' | 'with' | 'into' | 'over' \\\n",
    "          'on'\n",
    "    TO -> 'to'\n",
    "    N -> 'children' | 'names' | 'war' | 'air-raids' | 'house' | 'professor' \\\n",
    "          | 'story' | 'heart' | 'country' | 'post' | 'office' | 'railway' | 'station' \\\n",
    "          | 'miles' | 'wife' | 'housekeeper' | 'servants' | 'man' | 'face' | 'hair' \\\n",
    "          | 'head'\n",
    "    NNP -> 'london'\n",
    "    NN -> 'something'\n",
    "    Pos -> 'whose' | 'their' | 'his'\n",
    "    Pro -> 'peter' | 'susan' | 'edmund' | 'lucy' | 'they' | 'them' | 'he' \\\n",
    "           | 'ivy' | 'margaret' | 'betty' | 'macready'\n",
    "    CC -> 'and' | 'but'\n",
    "    Comma -> ','\n",
    "    Prep -> 'about'\n",
    "    WDT -> 'which'\n",
    "    JJS -> 'most'\n",
    "    \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the children were the children\n",
      "the children were the names\n",
      "the children were the war\n",
      "the children were the air-raids\n"
     ]
    }
   ],
   "source": [
    "## generating some senteces in non-random fashion\n",
    "\n",
    "for sentence in generate(cfg, n=4):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Checking if provided sentence matches the grammar:\n",
      "the professor was a very old man with shaggy white hair\n",
      "(S[]\n",
      "  (NP[] (Det[] the) (N[] professor))\n",
      "  (VP[]\n",
      "    (V[] was)\n",
      "    (NP[]\n",
      "      (NP[] (Det[] a) (ADJP[] (Adv[] very) (Adj[] old)) (N[] man))\n",
      "      (PP[]\n",
      "        (IN[] with)\n",
      "        (NP[]\n",
      "          (ADJP[]\n",
      "            (IterAdj[]\n",
      "              (IterAdj[] (Adj[] shaggy))\n",
      "              (IterAdj[] (Adj[] white))))\n",
      "          (N[] hair))))))\n",
      "(S[]\n",
      "  (NP[] (Det[] the) (N[] professor))\n",
      "  (VP[]\n",
      "    (V[] was)\n",
      "    (NP[]\n",
      "      (NP[] (Det[] a) (ADJP[] (Adv[] very) (Adj[] old)) (N[] man))\n",
      "      (PP[]\n",
      "        (IN[] with)\n",
      "        (NP[]\n",
      "          (ADJP[] (IterAdj[] (Adj[] shaggy)))\n",
      "          (NP[] (ADJP[] (IterAdj[] (Adj[] white))) (N[] hair)))))))\n",
      "(S[]\n",
      "  (NP[] (Det[] the) (N[] professor))\n",
      "  (VP[]\n",
      "    (V[] was)\n",
      "    (NP[] (Det[] a) (ADJP[] (Adv[] very) (Adj[] old)) (N[] man))\n",
      "    (PP[]\n",
      "      (IN[] with)\n",
      "      (NP[]\n",
      "        (ADJP[]\n",
      "          (IterAdj[]\n",
      "            (IterAdj[] (Adj[] shaggy))\n",
      "            (IterAdj[] (Adj[] white))))\n",
      "        (N[] hair)))))\n",
      "(S[]\n",
      "  (NP[] (Det[] the) (N[] professor))\n",
      "  (VP[]\n",
      "    (V[] was)\n",
      "    (NP[] (Det[] a) (ADJP[] (Adv[] very) (Adj[] old)) (N[] man))\n",
      "    (PP[]\n",
      "      (IN[] with)\n",
      "      (NP[]\n",
      "        (ADJP[] (IterAdj[] (Adj[] shaggy)))\n",
      "        (NP[] (ADJP[] (IterAdj[] (Adj[] white))) (N[] hair))))))\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if sentences can be parsed\n",
    "\n",
    "cfg_parser = FeatureEarleyChartParser(cfg)\n",
    "#check_sentence(cfg_parser, 'There were four children whose names were Peter , edmund , lucy and susan'.lower())\n",
    "#check_sentence(cfg_parser, 'this story is about something that happened to them'.lower())\n",
    "#check_sentence(cfg_parser, 'they were sent away from London during the war because of the air-raids'.lower())\n",
    "#check_sentence(cfg_parser, 'They were sent to the house of an old Professor'.lower())\n",
    "#check_sentence(cfg_parser, 'The Professor lived in the heart of the country'.lower())\n",
    "#check_sentence(cfg_parser, 'he lived ten miles from the nearest railway station and he lived two miles from the nearest post office'.lower())\n",
    "#check_sentence(cfg_parser, 'Their names were Ivy , Margaret and Betty , but they do not come into the story much'.lower())\n",
    "#check_sentence(cfg_parser, 'the housekeeper was a very old man'.lower())\n",
    "check_sentence(cfg_parser, 'The professor was a very old man with shaggy white hair'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
