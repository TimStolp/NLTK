{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taaltheorie & Taalverwerking 2018 - Assignment 6\n",
    "\n",
    "In this assignment, we will work more with WordNet and explore music processing with NLTK tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN FOR YOUR GROUP, also name your file as: tttv_ass6_<group>_<name1>_<name2>.ipynb\n",
    "# Group        : D\n",
    "# Name - UvaID : 11430087\n",
    "# Name - UvaID :\n",
    "# Date         : 30-05-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to load WordNet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/willem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import *\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (6pts total)\n",
    "\n",
    "In this exercise you will explore the output of a distributional semantic model and compare its semantic similarity ranking to that obtained with path-length distance in an ontology. \n",
    "\n",
    "#### Question 1.1 (1pt)\n",
    "Infomap is an implementation of a distributional semantic model that can be queried online at http://clic.cimec.unitn.it/infomap-query/. For a target word, it will return its $n$ nearest semantic neighbours ordered by similarity strength (calculated using cosine similarity between vectors in semantic space). Have a look at the model options that can be selected in the querying interface. Read the Infomap documentation (link at the end of the  page) and briefly summarise (in your own words) the features of the model option **bnc-lemma-narrow**. Your summary should include the language for which the model has been constructed, the corpus used, the characteristics of the target words (the rows), and the size of the context.\n",
    "\n",
    "#### Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bnc-lemma-narrow's has been trained on an english corpus of only nouns, verbs and adjectives. \n",
    "The corpus had all of its original words replaced with their corresponding lemma's. Furthermore, \n",
    "there were exactly 20000 target words with 2000 features. The context in which the target words occured \n",
    "were composed of sentences of only five words long with the target word occuring only in the middle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2 (2pts)\n",
    "Search for the target word **car** using the following parameters: target word PoS **noun**, model option **bnc-lemma-narrow**, neighbor PoS  **noun**, max. number of neighbors **5**. As output you should get a list of five words including **car**, with a similarity score that indicates how similar they are to **car** according to the model. \n",
    "\n",
    "Use NLTK to determine the lowest common hypernym of these five words in *WordNet*.\n",
    "\n",
    "**Hint:** Don't forget that you need to check all possible senses for each word. The lowest common hypernym is the lowest-level synset that subsumes at least one sense of all five words. You probably *don't* want to use the NLTK **lowest_common_hypernyms()** method. (Can you see why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('conveyance.n.03')\n"
     ]
    }
   ],
   "source": [
    "# Could have gone for something more efficient,\n",
    "# but this works more than fine and is simple.\n",
    "\n",
    "search_list = []\n",
    "\n",
    "car = wn.synsets('car')\n",
    "van = wn.synsets('van')\n",
    "vehicle = wn.synsets('vehicle')\n",
    "truck = wn.synsets('truck')\n",
    "motorcycle = wn.synsets('motorcycle')\n",
    "\n",
    "search_list.extend(car)\n",
    "search_list.extend(van)\n",
    "search_list.extend(vehicle)\n",
    "search_list.extend(truck)\n",
    "search_list.extend(motorcycle)\n",
    "\n",
    "# returns common lowest hypernym for a list of synsets\n",
    "def BFS (search_list):\n",
    "    while search_list:\n",
    "        element = search_list.pop(0)\n",
    "        search_list.extend(element.hypernyms())\n",
    "        hypos = set(s for s in element.closure(lambda s:s.hyponyms()))\n",
    "        if goal(hypos):\n",
    "            return element\n",
    "        \n",
    "    return False\n",
    "\n",
    "# true if hyponyms are present in all given sets\n",
    "def goal(hypos):\n",
    "    if (    not set.isdisjoint(hypos, set(car)) \\\n",
    "        and not set.isdisjoint(hypos, set(van)) \\\n",
    "        and not set.isdisjoint(hypos, set(vehicle)) \\\n",
    "        and not set.isdisjoint(hypos, set(truck)) \\\n",
    "        and not set.isdisjoint(hypos, set(motorcycle))):\n",
    "        return True\n",
    "        \n",
    "    \n",
    "print(BFS(search_list)) # lowest common hypernym is: 'conveyance.n.03'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3 (2pts)\n",
    "\n",
    "Implement a function called **co\\_hyponym(node1, node2)** to check whether two *words* (not synsets) are co-hyponyms or sister terms (i.e., whether they have some immediate hypernym in common). Which of the other words on your Infomap list are co-hyponyms of 'car'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "[(Synset('car.n.01'), Synset('truck.n.01'))]\n",
      "[(Synset('car.n.01'), Synset('motorcycle.n.01'))]\n"
     ]
    }
   ],
   "source": [
    "# Check if they have the same parent.\n",
    "\n",
    "def co_hyponym(word1, word2):\n",
    "    co_hypos = []\n",
    "    word1_synsets = wn.synsets(word1)\n",
    "    word2_synsets = wn.synsets(word2)\n",
    "    \n",
    "    if not word1_synsets or not word2_synsets:\n",
    "        print('Input not found')\n",
    "        return \n",
    "\n",
    "    for word1 in word1_synsets:\n",
    "        word1_hypers = set(word1.hypernyms())\n",
    "        for word2 in word2_synsets:\n",
    "            if not word1_hypers.isdisjoint(set(word2.hypernyms())):\n",
    "                co_hypos.append((word1, word2))\n",
    "    if co_hypos == []:\n",
    "        return False\n",
    "    else:\n",
    "        return co_hypos\n",
    "            \n",
    "            \n",
    "\n",
    "print(co_hyponym(\"car\", \"van\")) # not co-hyponyms\n",
    "print(co_hyponym(\"car\", \"vehicle\")) # not co-hyponyms\n",
    "print(co_hyponym(\"car\", \"truck\")) # co-hyponyms\n",
    "print(co_hyponym(\"car\", \"motorcycle\")) # co-hyponyms \n",
    "\n",
    "#print(wn.synsets('motorcyle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3 (1pt)\n",
    "Give the results of **textbook_similarity** queries between **car** and each of the other four words in this ontology, using your function from last week. Give the resulting list of semantic neighbours of **car** ordered by similarity strength (according to WordNet path-length) and compare this ranking to the one output by Infomap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def textbook_similarity(word1, word2, pos):\n",
    "    if word1 == word2:\n",
    "        print(\"inf\")\n",
    "        return \"\"\n",
    "    \n",
    "    word1_synsets = wn.synsets(word1, pos = pos)\n",
    "    word2_synsets = wn.synsets(word2, pos = pos)\n",
    "\n",
    "    #print('Word1 synsets: {}'.format(word1_synsets))\n",
    "    #print('Word2 synsets: {}'.format(word2_synsets))\n",
    "    for synset1 in word1_synsets:\n",
    "      if synset1 in word2_synsets:\n",
    "        print('Early return, similarity is infinity')\n",
    "        return\n",
    "\n",
    "    from itertools import chain\n",
    "    def is_similar(hypernyms1, hypernyms2):\n",
    "      similarities = []\n",
    "      for i in range(len(hypernyms1)):\n",
    "        for j in range(len(hypernyms2)):\n",
    "          for nym in hypernyms2[j]:\n",
    "            if nym in hypernyms1[i]:\n",
    "              return (i + j)\n",
    "      return None\n",
    "\n",
    "    nyms1 = word1_synsets\n",
    "    nyms2 = word2_synsets\n",
    "    word1_hypernyms = []  # 2D array\n",
    "    word2_hypernyms = []  # 2D array\n",
    "    while (\n",
    "            (\n",
    "              len(list(chain.from_iterable(nym.hypernyms() for nym in nyms1))) > 0 \n",
    "              or len(list(chain.from_iterable(nym.hypernyms() for nym in nyms2))) > 0 \n",
    "            )\n",
    "          ):\n",
    "      nyms1 = list(chain.from_iterable(nym.hypernyms() for nym in nyms1))\n",
    "      nyms2 = list(chain.from_iterable(nym.hypernyms() for nym in nyms2))\n",
    "      word1_hypernyms.append(nyms1)\n",
    "      word2_hypernyms.append(nyms2)\n",
    "      #print('At depth {}'.format(max([len(word1_hypernyms), len(word2_hypernyms)])))\n",
    "\n",
    "    if is_similar([word1_synsets]+word1_hypernyms, [word2_synsets]+word2_hypernyms) is not None:\n",
    "        print('{}'.format(is_similar([word1_synsets]+word1_hypernyms, [word2_synsets]+word2_hypernyms)))\n",
    "    else:\n",
    "        print('-inf')\n",
    "    return \"\"\n",
    "\n",
    "print(textbook_similarity(\"car\", \"van\", pos = wn.NOUN))\n",
    "print(textbook_similarity(\"car\", \"vehicle\", pos = wn.NOUN))\n",
    "print(textbook_similarity(\"car\", \"truck\", pos = wn.NOUN))\n",
    "print(textbook_similarity(\"car\", \"motorcycle\", pos = wn.NOUN))\n",
    "\n",
    "# The ranking is quite similar to the one given by infomap.\n",
    "# They mayor difference is that infomap doesn't rank vehicle, \n",
    "# truck and motorcycle as equally similar, in contrary\n",
    "# to the textbook similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 2: Parsing Eurovision (6pts total)\n",
    "\n",
    "This question will introduce the basics of grammar-based syntactic analysis of musical harmony.\n",
    "\n",
    "### Question 2.1 (1pt)\n",
    "\n",
    "Go to http://chordify.net and run an analysis of Australia's official entry for the Eurovision Song Contest 2018: https://www.youtube.com/watch?reload=9&v=J4XZxbrvepw. Copy the first six chords down as they appear in the Chordify interface, as a list of Python strings. You may find it slightly faster if you change to the *Akkorden* view instead of *Diagrammen*.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "australian_chords = [\"G\", \"E\", \"Em\", \"C\", \"D\", \"G\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 (2pts)\n",
    "\n",
    "The following grammar is a (somewhat simplified) implementation of the harmonic grammar Fred Lerdahl proposes in his book *Tonal Pitch Space* (2001). In addition to the traditional harmonic classes of tonic, dominant, and subdominant harmony, Lerdahl's grammar includes a *departure* class (`Dep`), a *return* class (`Ret`), and a *neighbour* class (`N`). His definitions of the harmonic classes in terms of Roman numerals are commented out: the 'lexical rules' for this grammar.\n",
    "\n",
    "The tonal centre of Australia's Eurovision entry is G. Use the principles we discussed in class to replace the Roman-numeral lexical rules with lexical rules for all of the chord symbols in `australian_chords` (e.g., `T -> 'G'`).\n",
    "\n",
    "Run the code block when you are finished to see how many parse trees Lerdahl's grammar can compute for this sentence.\n",
    "\n",
    "**Hint:** If you still find the rules for converting between chord names and Roman numerals confusing, try browsing Wikipedia's surprisingly good collection of articles on harmony, starting here: https://en.wikipedia.org/wiki/Roman_numeral_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import CFG\n",
    "from nltk.parse.chart import LeftCornerChartParser\n",
    "\n",
    "lerdahl_grammar = CFG.fromstring(\"\"\"\n",
    "  P -> T\n",
    "  T -> T T\n",
    "  T -> D T\n",
    "  D -> D D\n",
    "  D -> S D\n",
    "  S -> S S\n",
    "  T -> T N T\n",
    "  D -> D N D\n",
    "  S -> S N S\n",
    "  T -> T Dep\n",
    "  D -> D Dep\n",
    "  S -> S Dep\n",
    "  Dep -> N Dep\n",
    "  T -> Ret T\n",
    "  D -> Ret D\n",
    "  S -> Ret S\n",
    "  \n",
    "  # Replace the following strings with the actual chord symbols you need.\n",
    "  # \n",
    "  # T -> 'I'\n",
    "  # D -> 'V' | 'VII'\n",
    "  # S -> 'II' | 'III' | 'IV' | 'VI' | 'VII'\n",
    "  # Dep -> 'II' | 'III' | 'IV' | 'V' | 'VI' | 'VII' \n",
    "  # Ret -> 'II' | 'III' | 'IV' | 'V' | 'VI' | 'VII' \n",
    "  # N -> 'II' | 'III' | 'IV' | 'V' | 'VI' | 'VII' \n",
    "\"\"\")\n",
    "\n",
    "lerdahl_parser = LeftCornerChartParser(lerdahl_grammar)\n",
    "lerdahl_parses = lerdahl_parser.parse(australian_chords)\n",
    "\n",
    "# Print the total number of parses as well as the actual parse trees.\n",
    "lerdahl_sum = 0\n",
    "for t in lerdahl_parses: \n",
    "    lerdahl_sum = lerdahl_sum + 1\n",
    "    t.pretty_print()\n",
    "print(lerdahl_sum, 'trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3 (2pts)\n",
    "\n",
    "The next grammar is based on the foundations of Chordify (Bas de Haas, *Music Information Retrieval Based on Tonal Harmony*, 2012). Chordify uses a different set of harmonic classes: tonic, dominant, subdominant, and *tonic prolongation* (`TPG`). They also define the members of the traditional classes differently than Fred Lerdahl; theirs  are based on a beautiful branch of music theory known as neo-Riemannian theory. Replace the commented 'lexical rules' in this grammar with actual chord symbols from `australian_chords`, just like you did in the previous question, and run the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chordify_grammar = CFG.fromstring(\"\"\"\n",
    "  P -> PCPa\n",
    "  P -> PCPb\n",
    "  P -> HCP\n",
    "  P -> P P\n",
    "\n",
    "  PCPa -> D T | D D T\n",
    "  PCPa -> PCPa T\n",
    "  PCPb -> T D T\n",
    "  HCP -> T D\n",
    "  HCP -> T HCP\n",
    "  D -> S D\n",
    "  T -> TPG\n",
    "  \n",
    "  T -> T T\n",
    "  D -> D D\n",
    "  S -> S S\n",
    "  \n",
    "  # Replace the following strings with the actual chord symbols you need.\n",
    "  # \n",
    "  # T -> 'I'\n",
    "  # D -> 'V' | 'VII'\n",
    "  # S -> 'II' | 'IV' \n",
    "  # TPG -> 'III' | 'VI'\n",
    "\"\"\")\n",
    "\n",
    "chordify_parser = LeftCornerChartParser(chordify_grammar)\n",
    "chordify_parses = chordify_parser.parse(australian_chords)\n",
    "\n",
    "# Print the total number of parses as well as the actual parse trees.\n",
    "chordify_sum = 0\n",
    "for t in chordify_parses: \n",
    "    chordify_sum = chordify_sum + 1\n",
    "    t.pretty_print()\n",
    "print(chordify_sum, 'trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4 (1pt):\n",
    "\n",
    "Which grammar is more practical for daily use, Lerdahl's or Chordify's? Why?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
